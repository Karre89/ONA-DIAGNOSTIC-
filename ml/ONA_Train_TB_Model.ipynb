{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ONA Health - Train TB Detection Model\n",
    "\n",
    "This notebook trains a ResNet18 model on chest X-ray data for TB detection.\n",
    "\n",
    "**What this does:**\n",
    "1. Downloads Shenzhen + Montgomery TB datasets (~800 images)\n",
    "2. Fine-tunes pretrained ResNet18\n",
    "3. Exports to ONNX format\n",
    "4. Downloads ready-to-deploy model\n",
    "\n",
    "**Requirements:**\n",
    "- Enable GPU: Runtime -> Change runtime type -> T4 GPU\n",
    "- Time: ~30-60 minutes total\n",
    "\n",
    "**Just click Runtime -> Run all and wait!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1-header"
   },
   "source": [
    "## Step 1: Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "setup"
   },
   "source": "# Check GPU availability\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"WARNING: No GPU detected! Training will be slow.\")\n    print(\"Go to Runtime -> Change runtime type -> T4 GPU\")\n\n# Install dependencies (including onnxscript for PyTorch 2.x)\n!pip install onnx onnxruntime onnxscript -q\nprint(\"\\nDependencies installed!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2-header"
   },
   "source": [
    "## Step 2: Download TB Datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "download-data"
   },
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create data directories\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Downloading TB datasets...\")\n",
    "print(\"(This may take a few minutes)\\n\")\n",
    "\n",
    "# Download Shenzhen dataset from NIH\n",
    "# Source: https://lhncbc.nlm.nih.gov/LHC-downloads/downloads.html\n",
    "shenzhen_url = \"https://data.lhncbc.nlm.nih.gov/public/Tuberculosis-Chest-X-ray-Datasets/Shenzhen-Hospital-CXR-Set/CXR_png.zip\"\n",
    "montgomery_url = \"https://data.lhncbc.nlm.nih.gov/public/Tuberculosis-Chest-X-ray-Datasets/Montgomery-County-CXR-Set/MontgomerySet/CXR_png.zip\"\n",
    "\n",
    "def download_and_extract(url, name):\n",
    "    zip_path = DATA_DIR / f\"{name}.zip\"\n",
    "    extract_path = DATA_DIR / name\n",
    "    \n",
    "    if extract_path.exists():\n",
    "        print(f\"{name} already downloaded\")\n",
    "        return extract_path\n",
    "    \n",
    "    print(f\"Downloading {name}...\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(f\"Extracting {name}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            z.extractall(extract_path)\n",
    "        os.remove(zip_path)\n",
    "        print(f\"{name} ready!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {name}: {e}\")\n",
    "        print(\"Trying alternative method...\")\n",
    "        # Alternative: use gdown or manual instructions\n",
    "        raise\n",
    "    return extract_path\n",
    "\n",
    "# Try to download datasets\n",
    "try:\n",
    "    shenzhen_path = download_and_extract(shenzhen_url, \"shenzhen\")\n",
    "    montgomery_path = download_and_extract(montgomery_url, \"montgomery\")\n",
    "    print(\"\\nDatasets downloaded successfully!\")\n",
    "except:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MANUAL DOWNLOAD REQUIRED\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"The NIH server may be blocking automated downloads.\")\n",
    "    print(\"\\nPlease download manually:\")\n",
    "    print(\"1. Go to: https://lhncbc.nlm.nih.gov/LHC-downloads/dataset.html\")\n",
    "    print(\"2. Download 'Shenzhen Hospital X-ray Set' and 'Montgomery County X-ray Set'\")\n",
    "    print(\"3. Upload the zip files to this Colab session\")\n",
    "    print(\"4. Re-run this cell\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "alternative-download"
   },
   "source": [
    "# Alternative: Use Kaggle dataset (easier to download)\n",
    "# This cell provides a backup if NIH download fails\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if we need alternative download\n",
    "if not (Path(\"data/shenzhen\").exists() and Path(\"data/montgomery\").exists()):\n",
    "    print(\"Using Kaggle TB dataset as alternative...\")\n",
    "    print(\"\\nOption 1: Upload your Kaggle API key\")\n",
    "    print(\"Option 2: Use the synthetic data generator below\")\n",
    "    \n",
    "    # Create synthetic dataset for testing if real data unavailable\n",
    "    USE_SYNTHETIC = True  # Set to False if you have real data\n",
    "    \n",
    "    if USE_SYNTHETIC:\n",
    "        print(\"\\nGenerating synthetic training data for demo...\")\n",
    "        print(\"(For production, use real TB datasets)\")\n",
    "        \n",
    "        import numpy as np\n",
    "        from PIL import Image\n",
    "        \n",
    "        # Create directories\n",
    "        for split in ['train', 'val']:\n",
    "            for label in ['normal', 'tb']:\n",
    "                Path(f\"data/organized/{split}/{label}\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Generate synthetic chest X-ray-like images\n",
    "        def generate_synthetic_cxr(is_tb=False, size=224):\n",
    "            # Create base image (lung-like pattern)\n",
    "            img = np.random.normal(128, 30, (size, size)).astype(np.uint8)\n",
    "            \n",
    "            # Add lung field pattern\n",
    "            y, x = np.ogrid[:size, :size]\n",
    "            center_y, center_x = size // 2, size // 2\n",
    "            \n",
    "            # Left lung\n",
    "            left_lung = ((x - center_x + 40)**2 / 2000 + (y - center_y)**2 / 4000) < 1\n",
    "            # Right lung\n",
    "            right_lung = ((x - center_x - 40)**2 / 2000 + (y - center_y)**2 / 4000) < 1\n",
    "            \n",
    "            img[left_lung] = np.clip(img[left_lung] - 40, 0, 255)\n",
    "            img[right_lung] = np.clip(img[right_lung] - 40, 0, 255)\n",
    "            \n",
    "            if is_tb:\n",
    "                # Add TB-like opacities (upper lobe)\n",
    "                tb_y = center_y - 30 + np.random.randint(-10, 10)\n",
    "                tb_x = center_x + np.random.choice([-40, 40]) + np.random.randint(-10, 10)\n",
    "                tb_region = ((x - tb_x)**2 + (y - tb_y)**2) < (15 + np.random.randint(5, 15))**2\n",
    "                img[tb_region] = np.clip(img[tb_region] + 50 + np.random.randint(0, 30), 0, 255)\n",
    "            \n",
    "            return Image.fromarray(img, mode='L')\n",
    "        \n",
    "        # Generate training data\n",
    "        n_train_per_class = 300\n",
    "        n_val_per_class = 75\n",
    "        \n",
    "        print(f\"Generating {n_train_per_class * 2} training images...\")\n",
    "        for i in range(n_train_per_class):\n",
    "            # Normal\n",
    "            img = generate_synthetic_cxr(is_tb=False)\n",
    "            img.save(f\"data/organized/train/normal/normal_{i:04d}.png\")\n",
    "            # TB\n",
    "            img = generate_synthetic_cxr(is_tb=True)\n",
    "            img.save(f\"data/organized/train/tb/tb_{i:04d}.png\")\n",
    "        \n",
    "        print(f\"Generating {n_val_per_class * 2} validation images...\")\n",
    "        for i in range(n_val_per_class):\n",
    "            # Normal\n",
    "            img = generate_synthetic_cxr(is_tb=False)\n",
    "            img.save(f\"data/organized/val/normal/normal_{i:04d}.png\")\n",
    "            # TB\n",
    "            img = generate_synthetic_cxr(is_tb=True)\n",
    "            img.save(f\"data/organized/val/tb/tb_{i:04d}.png\")\n",
    "        \n",
    "        print(\"\\nSynthetic data generated!\")\n",
    "        print(f\"  Training: {n_train_per_class * 2} images\")\n",
    "        print(f\"  Validation: {n_val_per_class * 2} images\")\n",
    "        print(\"\\nNote: This is synthetic data for demo purposes.\")\n",
    "        print(\"For production, train on real Shenzhen/Montgomery data.\")\n",
    "else:\n",
    "    print(\"Real datasets available, skipping synthetic generation.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3-header"
   },
   "source": [
    "## Step 3: Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "data-loaders"
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])  # Grayscale normalization\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "\n",
    "class TBDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        # Load normal images (label=0)\n",
    "        normal_dir = self.root_dir / \"normal\"\n",
    "        if normal_dir.exists():\n",
    "            for img_path in normal_dir.glob(\"*.png\"):\n",
    "                self.samples.append((img_path, 0))\n",
    "            for img_path in normal_dir.glob(\"*.jpg\"):\n",
    "                self.samples.append((img_path, 0))\n",
    "        \n",
    "        # Load TB images (label=1)\n",
    "        tb_dir = self.root_dir / \"tb\"\n",
    "        if tb_dir.exists():\n",
    "            for img_path in tb_dir.glob(\"*.png\"):\n",
    "                self.samples.append((img_path, 1))\n",
    "            for img_path in tb_dir.glob(\"*.jpg\"):\n",
    "                self.samples.append((img_path, 1))\n",
    "        \n",
    "        random.shuffle(self.samples)\n",
    "        print(f\"Loaded {len(self.samples)} images from {root_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image as grayscale\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        \n",
    "        # Convert to 3-channel for ResNet (expects RGB)\n",
    "        image = Image.merge('RGB', (image, image, image))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TBDataset(\"data/organized/train\", transform=train_transform)\n",
    "val_dataset = TBDataset(\"data/organized/val\", transform=val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"\\nData loaders ready:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4-header"
   },
   "source": [
    "## Step 4: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "create-model"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class TBClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    TB Detection Model based on ResNet18\n",
    "    \n",
    "    Input: 224x224x3 image (grayscale converted to RGB)\n",
    "    Output: 2 values (normal, TB probabilities)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained ResNet18\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Replace final layer for binary classification\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TBClassifier(num_classes=2)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model created on: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5-header"
   },
   "source": [
    "## Step 5: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "train"
   },
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "# Training history\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "print(f\"Training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.1f}%\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"=\"*60)\n",
    "print(f\"Training complete in {elapsed_time/60:.1f} minutes\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.1f}%\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)\n",
    "print(\"\\nLoaded best model weights.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6-header"
   },
   "source": [
    "## Step 6: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "evaluate"
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())  # TB probability\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(all_labels, all_preds, target_names=['Normal', 'TB']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Normal    TB\")\n",
    "print(f\"Actual Normal   {cm[0,0]:4d}    {cm[0,1]:4d}\")\n",
    "print(f\"Actual TB       {cm[1,0]:4d}    {cm[1,1]:4d}\")\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn) * 100\n",
    "specificity = tn / (tn + fp) * 100\n",
    "\n",
    "print(f\"\\nSensitivity (TB detection): {sensitivity:.1f}%\")\n",
    "print(f\"Specificity (Normal detection): {specificity:.1f}%\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['val_acc'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining history saved to training_history.png\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step7-header"
   },
   "source": [
    "## Step 7: Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "export-onnx"
   },
   "source": "import torch\nimport onnx\nimport os\nfrom onnx.external_data_helper import convert_model_to_external_data\n\n# Configuration\nMODEL_VERSION = \"v2.0\"\nOUTPUT_FILE = f\"ona-cxr-resnet18-{MODEL_VERSION}.onnx\"\n\n# Set model to eval mode\nmodel.eval()\nmodel = model.cpu()  # Move to CPU for export\n\n# Create dummy input\ndummy_input = torch.randn(1, 3, 224, 224)\n\n# Export to ONNX (may create split files)\nprint(f\"Exporting to {OUTPUT_FILE}...\")\ntemp_file = \"temp_model.onnx\"\ntorch.onnx.export(\n    model,\n    dummy_input,\n    temp_file,\n    export_params=True,\n    opset_version=17,\n    do_constant_folding=True,\n    input_names=['image'],\n    output_names=['logits'],\n    dynamic_axes={\n        'image': {0: 'batch_size'},\n        'logits': {0: 'batch_size'}\n    }\n)\n\n# Load the model and save as single file (combine external data)\nprint(\"Combining into single file...\")\nonnx_model = onnx.load(temp_file, load_external_data=True)\nonnx.save_model(onnx_model, OUTPUT_FILE, save_as_external_data=False)\n\n# Clean up temp files\nif os.path.exists(temp_file):\n    os.remove(temp_file)\nif os.path.exists(temp_file + \".data\"):\n    os.remove(temp_file + \".data\")\n\nfile_size = os.path.getsize(OUTPUT_FILE)\nprint(f\"\\n✓ Export complete!\")\nprint(f\"  File: {OUTPUT_FILE}\")\nprint(f\"  Size: {file_size / 1024 / 1024:.1f} MB\")\nprint(f\"\\n✓ Single file export - no .data file needed!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step8-header"
   },
   "source": [
    "## Step 8: Verify ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "verify-onnx"
   },
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load and verify ONNX model\n",
    "print(\"Verifying ONNX model...\")\n",
    "onnx_model = onnx.load(OUTPUT_FILE)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"✓ Model structure valid\")\n",
    "\n",
    "# Test with ONNX Runtime\n",
    "session = ort.InferenceSession(OUTPUT_FILE, providers=['CPUExecutionProvider'])\n",
    "input_info = session.get_inputs()[0]\n",
    "output_info = session.get_outputs()[0]\n",
    "\n",
    "print(f\"\\nONNX Model Info:\")\n",
    "print(f\"  Input:  {input_info.name} - {input_info.shape}\")\n",
    "print(f\"  Output: {output_info.name} - {output_info.shape}\")\n",
    "\n",
    "# Run inference with ONNX Runtime\n",
    "print(\"\\nTesting inference...\")\n",
    "test_input = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
    "onnx_output = session.run(None, {input_info.name: test_input})[0]\n",
    "\n",
    "# Compare with PyTorch output\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pytorch_output = model(torch.from_numpy(test_input)).numpy()\n",
    "\n",
    "max_diff = np.abs(pytorch_output - onnx_output).max()\n",
    "print(f\"PyTorch vs ONNX max difference: {max_diff:.8f}\")\n",
    "\n",
    "if max_diff < 0.0001:\n",
    "    print(\"✓ Verification PASSED - outputs match!\")\n",
    "else:\n",
    "    print(\"⚠ Warning: outputs differ slightly (should still work)\")\n",
    "\n",
    "# Test with actual inference\n",
    "print(\"\\nSample inference test:\")\n",
    "probs = np.exp(onnx_output) / np.exp(onnx_output).sum()  # Softmax\n",
    "print(f\"  Normal probability: {probs[0][0]:.2%}\")\n",
    "print(f\"  TB probability: {probs[0][1]:.2%}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step9-header"
   },
   "source": [
    "## Step 9: Create Manifest & Download"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "create-manifest"
   },
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create manifest\n",
    "manifest = {\n",
    "    \"model_name\": f\"ona-cxr-resnet18-{MODEL_VERSION}\",\n",
    "    \"model_file\": OUTPUT_FILE,\n",
    "    \"version\": MODEL_VERSION,\n",
    "    \"architecture\": \"ResNet18\",\n",
    "    \"framework\": \"PyTorch (vanilla)\",\n",
    "    \"created\": datetime.now().isoformat(),\n",
    "    \"input_shape\": [1, 3, 224, 224],\n",
    "    \"input_format\": \"RGB (grayscale duplicated to 3 channels)\",\n",
    "    \"output_shape\": [1, 2],\n",
    "    \"output_format\": \"logits [normal, tb]\",\n",
    "    \"classes\": [\"normal\", \"tb\"],\n",
    "    \"preprocessing\": {\n",
    "        \"resize\": [224, 224],\n",
    "        \"normalize_mean\": [0.485, 0.485, 0.485],\n",
    "        \"normalize_std\": [0.229, 0.229, 0.229]\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"best_val_accuracy\": best_val_acc,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity\n",
    "    },\n",
    "    \"file_size_mb\": round(file_size / 1024 / 1024, 2)\n",
    "}\n",
    "\n",
    "manifest_file = f\"ona-cxr-resnet18-{MODEL_VERSION}.manifest.json\"\n",
    "with open(manifest_file, 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Manifest saved: {manifest_file}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING & EXPORT COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFiles ready for download:\")\n",
    "print(f\"  1. {OUTPUT_FILE} ({file_size / 1024 / 1024:.1f} MB)\")\n",
    "print(f\"  2. {manifest_file}\")\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Accuracy: {best_val_acc:.1f}%\")\n",
    "print(f\"  Sensitivity: {sensitivity:.1f}%\")\n",
    "print(f\"  Specificity: {specificity:.1f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "download"
   },
   "source": [
    "# Download files\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Starting downloads...\")\n",
    "print(\"(Check your browser's download bar)\")\n",
    "\n",
    "files.download(OUTPUT_FILE)\n",
    "files.download(manifest_file)\n",
    "\n",
    "print(\"\\n✓ Downloads started!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "1. Wait for downloads to complete\n",
    "\n",
    "2. Copy model to your edge agent:\n",
    "   - File: {OUTPUT_FILE}\n",
    "   - Location: edge-agent/data/models/\n",
    "\n",
    "3. The edge agent will auto-detect and use the model\n",
    "\n",
    "4. Test with:\n",
    "   curl -X POST http://localhost:8080/api/ingest-sample\n",
    "\n",
    "Congratulations! You now have a real TB detection model!\n",
    "\"\"\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## Notes\n",
    "\n",
    "### Model Details\n",
    "- **Architecture**: ResNet18 (pretrained on ImageNet)\n",
    "- **Input**: 224x224 RGB image (grayscale X-ray duplicated to 3 channels)\n",
    "- **Output**: 2 logits [normal, tb] - apply softmax for probabilities\n",
    "\n",
    "### For Production Use\n",
    "- Train on real Shenzhen + Montgomery datasets\n",
    "- Consider larger model (ResNet50) for better accuracy\n",
    "- Add more data augmentation\n",
    "- Implement proper cross-validation\n",
    "\n",
    "### Deployment\n",
    "```python\n",
    "# Edge agent preprocessing\n",
    "image = load_grayscale(path)\n",
    "image = resize(image, (224, 224))\n",
    "image = np.stack([image, image, image], axis=0)  # RGB\n",
    "image = (image / 255.0 - 0.485) / 0.229  # Normalize\n",
    "image = image[np.newaxis, ...]  # Add batch dim\n",
    "\n",
    "# Run inference\n",
    "logits = onnx_session.run(None, {'image': image})[0]\n",
    "probs = softmax(logits)\n",
    "tb_probability = probs[0][1]\n",
    "```"
   ]
  }
 ]
}